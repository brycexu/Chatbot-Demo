{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM09BNzoS7g696rBYDSC7O+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uiJ3TjOaRTZz"},"outputs":[],"source":["!pip install transformers[torch]\n","!pip install pandas\n","!pip install datasets\n","!pip install pyarrow\n","!pip install scikit-learn"]},{"cell_type":"code","source":["# Model: text classification model \"bert-base-uncased\"\n","# Dataset: review-sentiment\n","# Task: classify reviews based on their sentiment"],"metadata":{"id":"_60gJZFsT8wk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the data"],"metadata":{"id":"FkMBvUaPSOQv"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv('/content/dataset.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"rYOFK5_pSNfT","executionInfo":{"status":"ok","timestamp":1676502647205,"user_tz":480,"elapsed":1184,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"5c4c44dc-6925-48a8-eb89-d66a7e59f88c"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"],"text/html":["\n","  <div id=\"df-c2f4b1ce-e4a2-4275-ae1e-ed049f301ae5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2f4b1ce-e4a2-4275-ae1e-ed049f301ae5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c2f4b1ce-e4a2-4275-ae1e-ed049f301ae5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c2f4b1ce-e4a2-4275-ae1e-ed049f301ae5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["## Process the data"],"metadata":{"id":"fyDoJ7BUTFJl"}},{"cell_type":"markdown","source":["The data should be in the format of: \n","+ input_ids\n","+ token_type_ids\n","+ attention_mask\n","+ label\n","+ text"],"metadata":{"id":"hgHHeQbZVsIA"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","\n","def process_data(row):\n","    text = row['review']\n","    text = str(text)\n","    text = ' '.join(text.split())\n","    encodings = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n","    label = 0\n","    if row['sentiment'] == 'positive':\n","        label += 1\n","    encodings['label'] = label\n","    encodings['text'] = text\n","    return encodings\n","\n","processed_data = []\n","for i in range(len(df[:5000])):\n","    processed_data.append(process_data(df.iloc[i]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2X6XayBGTG-x","executionInfo":{"status":"ok","timestamp":1676503192089,"user_tz":480,"elapsed":13080,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"61ffe37a-c0f4-4a36-b338-d883cf83cb28"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n"]}]},{"cell_type":"markdown","source":["## Generate the dataset"],"metadata":{"id":"0BWI3gcTTK4X"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import pyarrow as pa\n","from datasets import Dataset\n","\n","new_df = pd.DataFrame(processed_data)\n","train_df, valid_df = train_test_split(\n","    new_df,\n","    test_size=0.2,\n","    random_state=2022\n",")\n","train_hg = Dataset(pa.Table.from_pandas(train_df))\n","valid_hg = Dataset(pa.Table.from_pandas(valid_df))"],"metadata":{"id":"jTAS9k5yTMk9","executionInfo":{"status":"ok","timestamp":1676503195716,"user_tz":480,"elapsed":297,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Define the model"],"metadata":{"id":"zpSJobfxUnnX"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=2\n",")\n","\n","training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"epoch\")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_hg,\n","    eval_dataset=valid_hg,\n","    tokenizer=tokenizer\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNcUwN4MUqY_","executionInfo":{"status":"ok","timestamp":1676503199453,"user_tz":480,"elapsed":1901,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"b69a4e5e-b891-4c0e-fecb-b0d22ee87747"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"markdown","source":["## Train the model"],"metadata":{"id":"d2N-uy2kVH3L"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"D_AKnEISVJQo","executionInfo":{"status":"ok","timestamp":1676503558886,"user_tz":480,"elapsed":356551,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"92161eb7-9c32-4fe6-b9ee-8369c66ce544"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 4000\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1500\n","  Number of trainable parameters = 109483778\n","You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 05:56, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.473200</td>\n","      <td>0.312430</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.256700</td>\n","      <td>0.494823</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.092100</td>\n","      <td>0.688378</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./result/checkpoint-500\n","Configuration saved in ./result/checkpoint-500/config.json\n","Model weights saved in ./result/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1000\n","Configuration saved in ./result/checkpoint-1000/config.json\n","Model weights saved in ./result/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","Saving model checkpoint to ./result/checkpoint-1500\n","Configuration saved in ./result/checkpoint-1500/config.json\n","Model weights saved in ./result/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./result/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./result/checkpoint-1500/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1500, training_loss=0.27398329162597657, metrics={'train_runtime': 356.3984, 'train_samples_per_second': 33.67, 'train_steps_per_second': 4.209, 'total_flos': 789333166080000.0, 'train_loss': 0.27398329162597657, 'epoch': 3.0})"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["## Save the model"],"metadata":{"id":"LdNFFF3EZXSu"}},{"cell_type":"code","source":["model.save_pretrained('./model/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjlyvW4BWgoz","executionInfo":{"status":"ok","timestamp":1676503573424,"user_tz":480,"elapsed":2238,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"18912002-6432-4bfa-c1ac-d96caba018b2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Configuration saved in ./model/config.json\n","Model weights saved in ./model/pytorch_model.bin\n"]}]},{"cell_type":"markdown","source":["## Load the model"],"metadata":{"id":"6XiTjsfiZa6y"}},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","new_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","new_model = AutoModelForSequenceClassification.from_pretrained('./model/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggKfoH4rZcKg","executionInfo":{"status":"ok","timestamp":1676503583186,"user_tz":480,"elapsed":2573,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"6e510ebd-ba5a-4420-d045-f3065cd8740f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading configuration file ./model/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"./model/\",\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file ./model/pytorch_model.bin\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./model/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"markdown","source":["## Evaluate the model"],"metadata":{"id":"rW3vdNAPcUwb"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import random\n","\n","new_model.cuda()\n","\n","def check(text, label):\n","    encoding = new_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n","    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n","    outputs = new_model(**encoding)\n","    logits = outputs.logits\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(logits.squeeze().cpu())\n","    probs = probs.detach().numpy()\n","    pred = np.argmax(probs, axis=-1)\n","    if pred == 1:\n","      return label == 'positive'\n","    else:\n","      return label == 'negative'\n","\n","num_samples = 1000\n","sample_idxes = random.sample(range(0, 15000), num_samples)\n","correct = 0\n","for i in sample_idxes:\n","    if check(df.iloc[i]['review'], df.iloc[i]['sentiment']):\n","      correct += 1\n","\n","print(\"Accuracy: \" + str(100*correct / num_samples) + \"%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpNhKKRpcYHe","executionInfo":{"status":"ok","timestamp":1676504577413,"user_tz":480,"elapsed":17066,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"86332179-78ca-4b6c-a935-6767aa7eaaeb"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 89.2%\n"]}]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"e_AQ93Y_ZpJk"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","def pred(text):\n","    encoding = new_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n","    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n","    outputs = new_model(**encoding)\n","    logits = outputs.logits\n","    sigmoid = torch.nn.Sigmoid()\n","    probs = sigmoid(logits.squeeze().cpu())\n","    probs = probs.detach().numpy()\n","    label = np.argmax(probs, axis=-1)\n","    if label == 1:\n","        return {\n","            'sentiment': 'Positive',\n","            'probability': probs[1]\n","        }\n","    else:\n","        return {\n","            'sentiment': 'Negative',\n","            'probability': probs[0]\n","        }\n","  \n","pred(\"I am not happy today\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpNoDdcmZrC1","executionInfo":{"status":"ok","timestamp":1676504619927,"user_tz":480,"elapsed":124,"user":{"displayName":"XIANDA XU","userId":"10196953528079152207"}},"outputId":"591941a2-22e0-429e-a589-c14479d5343f"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'sentiment': 'Negative', 'probability': 0.95608884}"]},"metadata":{},"execution_count":41}]}]}